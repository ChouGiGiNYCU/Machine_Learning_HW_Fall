{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"UinMOP1F6A2p","outputId":"0ce5c266-8e4c-4b71-c2d4-3ffa3c3727d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.0.1\n","cuda:0\n"]}],"source":["import os\n","import torch\n","import pandas as pd\n","import h5py\n","import Levenshtein\n","from torch.utils.data import DataLoader\n","from torch.utils.data import TensorDataset\n","from Levenshtein import distance\n","from transformers import MarianMTModel, MarianTokenizer\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","from torch.optim.lr_scheduler import StepLR\n","\n","print(torch.__version__)\n","#GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","#Assuming that we are on a CUDA machine, this should print a CUDA device:\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KXCfgHO16A2u"},"outputs":[],"source":["pretrain_model = \"Helsinki-NLP/opus-mt-zh-en\"\n","tokenizer = MarianTokenizer.from_pretrained(pretrain_model)\n","model = MarianMTModel.from_pretrained(pretrain_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-dGheV206A2v"},"outputs":[],"source":["def levenshtein_distance(str1, str2): #loss\n","    return distance(str1, str2)\n","\n","TrainData_out = pd.read_csv('/mount/ml/hw3/data/train-TL.csv')\n","TrainData_in = pd.read_csv('/mount/ml/hw3/data/train-ZH.csv')\n","TestDate = pd.read_csv('/mount/ml/hw3/data/test-ZH-nospace.csv')\n","\n","TrainData_in=TrainData_in['txt']\n","TrainData_out=TrainData_out['txt']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wYu6eTOf6A2w"},"outputs":[],"source":["input_encoded_texts = []\n","max_length_zh=0\n","#find max_length_zh\n","for text in TrainData_in:\n","    encoded_text = tokenizer.encode(\n","        text,\n","        add_special_tokens=True,  #add bos eos\n","        return_tensors='pt'       #return PyTorch tensor\n","    )\n","    max_length_zh=max(max_length_zh,encoded_text.size(1))\n","#print(\"max_length_zh : \",max_length_zh)\n","\n","for text in TrainData_in:\n","    encoded_text = tokenizer.encode(\n","        text,\n","        add_special_tokens=True,\n","        padding='max_length',\n","        truncation=True,\n","        max_length=max_length_zh,\n","        return_tensors='pt'\n","    )\n","    input_encoded_texts.append(encoded_text)\n","\n","input_ch = torch.cat(input_encoded_texts, dim=0).clone().detach()\n","attention_mask = (input_ch != 65000).to(torch.long)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1YXg-4JC6A2w"},"outputs":[],"source":["output_encoded_texts = []\n","max_length_zh=0\n","#find max_length_zh\n","for text in TrainData_out:\n","    encoded_text = tokenizer.encode(\n","        text,\n","        add_special_tokens=True,  #add bos eos\n","        return_tensors='pt'       #return PyTorch tensor\n","    )\n","    max_length_zh=max(max_length_zh,encoded_text.size(1))\n","#print(\"max_length_zh : \",max_length_zh)\n","\n","for text in TrainData_out:\n","    encoded_text = tokenizer.encode(\n","        text,\n","        add_special_tokens=True,\n","        padding='max_length',\n","        truncation=True,\n","        max_length=max_length_zh,\n","        return_tensors='pt'\n","    )\n","    output_encoded_texts.append(encoded_text)\n","\n","output_Eng = torch.cat(output_encoded_texts, dim=0).clone().detach()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wXOXn3CP6A2x"},"outputs":[],"source":["trainDataset = TensorDataset(input_ch, attention_mask, output_Eng)\n","trainDataloader = DataLoader(trainDataset, batch_size=8, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dp6_SZv06A2x","outputId":"7b04b51c-9cf9-4bdf-a1b8-0d2b3ce8a800"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                               \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/90, Mean Levenshtein Distance: 13.4365\n"]},{"name":"stderr","output_type":"stream","text":["                                                               \r"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/mount/ml/hw3/5.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4261754368695f7079746f726368227d@ssh-remote%2B140.113.193.7/mount/ml/hw3/5.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m: input_ch\u001b[39m.\u001b[39mto(device), \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m: attention_mask\u001b[39m.\u001b[39mto(device)}\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4261754368695f7079746f726368227d@ssh-remote%2B140.113.193.7/mount/ml/hw3/5.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, labels\u001b[39m=\u001b[39moutput_Eng\u001b[39m.\u001b[39mto(device))\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4261754368695f7079746f726368227d@ssh-remote%2B140.113.193.7/mount/ml/hw3/5.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m predictions \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mbatch_decode(torch\u001b[39m.\u001b[39;49margmax(output\u001b[39m.\u001b[39;49mlogits, dim\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m), skip_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4261754368695f7079746f726368227d@ssh-remote%2B140.113.193.7/mount/ml/hw3/5.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m tgt \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(output_Eng, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4261754368695f7079746f726368227d@ssh-remote%2B140.113.193.7/mount/ml/hw3/5.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m batch_distance \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(levenshtein_distance(pred, target) \u001b[39mfor\u001b[39;00m pred, target \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(predictions, tgt))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:243\u001b[0m, in \u001b[0;36mMarianTokenizer.batch_decode\u001b[0;34m(self, sequences, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_decode\u001b[39m(\u001b[39mself\u001b[39m, sequences, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    223\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mbatch_decode(sequences, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3706\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[0;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3682\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_decode\u001b[39m(\n\u001b[1;32m   3683\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   3684\u001b[0m     sequences: Union[List[\u001b[39mint\u001b[39m], List[List[\u001b[39mint\u001b[39m]], \u001b[39m\"\u001b[39m\u001b[39mnp.ndarray\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtorch.Tensor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtf.Tensor\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3687\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3688\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m   3689\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3690\u001b[0m \u001b[39m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3691\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3704\u001b[0m \u001b[39m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3705\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3706\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m   3707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(\n\u001b[1;32m   3708\u001b[0m             seq,\n\u001b[1;32m   3709\u001b[0m             skip_special_tokens\u001b[39m=\u001b[39mskip_special_tokens,\n\u001b[1;32m   3710\u001b[0m             clean_up_tokenization_spaces\u001b[39m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[1;32m   3711\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3712\u001b[0m         )\n\u001b[1;32m   3713\u001b[0m         \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences\n\u001b[1;32m   3714\u001b[0m     ]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3707\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3682\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_decode\u001b[39m(\n\u001b[1;32m   3683\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   3684\u001b[0m     sequences: Union[List[\u001b[39mint\u001b[39m], List[List[\u001b[39mint\u001b[39m]], \u001b[39m\"\u001b[39m\u001b[39mnp.ndarray\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtorch.Tensor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtf.Tensor\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3687\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3688\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m   3689\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3690\u001b[0m \u001b[39m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3691\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3704\u001b[0m \u001b[39m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3705\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3706\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m-> 3707\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecode(\n\u001b[1;32m   3708\u001b[0m             seq,\n\u001b[1;32m   3709\u001b[0m             skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens,\n\u001b[1;32m   3710\u001b[0m             clean_up_tokenization_spaces\u001b[39m=\u001b[39;49mclean_up_tokenization_spaces,\n\u001b[1;32m   3711\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3712\u001b[0m         )\n\u001b[1;32m   3713\u001b[0m         \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences\n\u001b[1;32m   3714\u001b[0m     ]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:269\u001b[0m, in \u001b[0;36mMarianTokenizer.decode\u001b[0;34m(self, token_ids, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, token_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    246\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[39m    Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[39m    tokens and clean up tokenization spaces.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39m        `str`: The decoded sentence.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdecode(token_ids, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3746\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3743\u001b[0m \u001b[39m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3744\u001b[0m token_ids \u001b[39m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3746\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode(\n\u001b[1;32m   3747\u001b[0m     token_ids\u001b[39m=\u001b[39;49mtoken_ids,\n\u001b[1;32m   3748\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens,\n\u001b[1;32m   3749\u001b[0m     clean_up_tokenization_spaces\u001b[39m=\u001b[39;49mclean_up_tokenization_spaces,\n\u001b[1;32m   3750\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3751\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils.py:1024\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         current_sub_text\u001b[39m.\u001b[39mappend(token)\n\u001b[1;32m   1023\u001b[0m \u001b[39mif\u001b[39;00m current_sub_text:\n\u001b[0;32m-> 1024\u001b[0m     sub_texts\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_tokens_to_string(current_sub_text))\n\u001b[1;32m   1026\u001b[0m \u001b[39mif\u001b[39;00m spaces_between_special_tokens:\n\u001b[1;32m   1027\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(sub_texts)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:278\u001b[0m, in \u001b[0;36mMarianTokenizer.convert_tokens_to_string\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    275\u001b[0m out_string \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens:\n\u001b[1;32m    277\u001b[0m     \u001b[39m# make sure that special tokens are not decoded using sentencepiece model\u001b[39;00m\n\u001b[0;32m--> 278\u001b[0m     \u001b[39mif\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_special_tokens:\n\u001b[1;32m    279\u001b[0m         out_string \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m sp_model\u001b[39m.\u001b[39mdecode_pieces(current_sub_tokens) \u001b[39m+\u001b[39m token \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         current_sub_tokens \u001b[39m=\u001b[39m []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1340\u001b[0m, in \u001b[0;36mSpecialTokensMixin.all_special_tokens\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1333\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m   1334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mall_special_tokens\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m   1335\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[39m    `List[str]`: A list of the unique special tokens (`'<unk>'`, `'<cls>'`, ..., etc.).\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \n\u001b[1;32m   1338\u001b[0m \u001b[39m    Convert tokens of `tokenizers.AddedToken` type to string.\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m     all_toks \u001b[39m=\u001b[39m [\u001b[39mstr\u001b[39m(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_special_tokens_extended]\n\u001b[1;32m   1341\u001b[0m     \u001b[39mreturn\u001b[39;00m all_toks\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1329\u001b[0m, in \u001b[0;36mSpecialTokensMixin.all_special_tokens_extended\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1328\u001b[0m         tokens_to_add \u001b[39m=\u001b[39m [value] \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(value) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m seen \u001b[39melse\u001b[39;00m []\n\u001b[0;32m-> 1329\u001b[0m     seen\u001b[39m.\u001b[39;49mupdate(\u001b[39mmap\u001b[39;49m(\u001b[39mstr\u001b[39;49m, tokens_to_add))\n\u001b[1;32m   1330\u001b[0m     all_tokens\u001b[39m.\u001b[39mextend(tokens_to_add)\n\u001b[1;32m   1331\u001b[0m \u001b[39mreturn\u001b[39;00m all_tokens\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n","scheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n","num_epochs = 90\n","model.to(device)\n","model.train()\n","epoch_list = []\n","MLD_list = []\n","for epoch in range(num_epochs):\n","    total_distance = 0\n","    total_samples = 0\n","    for batch in tqdm(trainDataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n","        input_ch, attention_mask, output_Eng = batch\n","        optimizer.zero_grad()\n","        input = {\"input_ids\": input_ch.to(device), \"attention_mask\": attention_mask.to(device)}\n","        output = model(**input, labels=output_Eng.to(device))\n","        predictions = tokenizer.batch_decode(torch.argmax(output.logits, dim=2), skip_special_tokens=True)\n","        tgt = tokenizer.batch_decode(output_Eng, skip_special_tokens=True)\n","        batch_distance = sum(levenshtein_distance(pred, target) for pred, target in zip(predictions, tgt))\n","        total_distance += batch_distance\n","        total_samples += len(predictions)\n","        loss = output.loss\n","        loss.backward()\n","        optimizer.step()\n","    mean_distance = total_distance / total_samples\n","    scheduler.step()\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Mean Levenshtein Distance: {mean_distance:.4f}\")\n","    MLD_list.append(mean_distance)\n","    epoch_list.append(epoch+1)\n","\n","    if (epoch + 1) % 10 == 0:  # save every 5 epoch\n","            save_dir = '/mount/ml/hw3/model_ver'\n","            os.makedirs(save_dir, exist_ok=True)\n","            PATH = os.path.join(save_dir, 'model_epoch_{}.n.pth'.format(epoch + 1))\n","            torch.save(model.state_dict(), PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wUCwfrLu6A2y"},"outputs":[],"source":["#load pth\n","'''\n","PATH = '/mount/ml/hw3/model_ver/model_epoch_5.n.pth'\n","model.load_state_dict(torch.load(PATH))\n","model = model.to(device)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"baavQMjN6A2z","outputId":"39434320-07fd-470f-db04-2ad66215d604"},"outputs":[{"name":"stderr","output_type":"stream","text":["翻譯: 100%|██████████| 641/641 [02:05<00:00,  5.09文本/s]\n"]}],"source":["tokenizer.save_pretrained(\"tokenizer\")\n","model.eval()\n","result = []\n","with torch.no_grad():\n","    for i in tqdm(range(len(TestDate)), desc=\"翻譯\", unit=\"文本\"):\n","        Eng_text = TestDate['txt'][i]\n","        inputs = tokenizer(Eng_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n","        out_id = model.generate(**inputs)\n","        precout_text = tokenizer.batch_decode(out_id, skip_special_tokens=True)[0]\n","        result.append(precout_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LMoRf6CP6A20"},"outputs":[],"source":["#output\n","idx=[]\n","for i in range(len(TestDate)):\n","    idx.append(i+1)\n","\n","df = pd.DataFrame({'id': idx, 'txt': result})\n","df.to_csv('/mount/ml/hw3/output.csv', index=False)\n","print(\"All complete\")"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}